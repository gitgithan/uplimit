{
  "podcast_details": {
    "podcast_title": "The AI in Business Podcast",
    "episode_title": "[AI Futures] Intensive vs. Laissez-Faire Approaches to AI Governance - with Robin Hanson of the Future of Humanity Institute at Oxford University",
    "episode_image": "https://ssl-static.libsyn.com/p/assets/1/c/0/2/1c02772256d769adbafc7308ab683e82/The_AI_in_business_new_Blue_32x.jpeg",
    "episode_transcript": " This is Daniel Fijell, Head of Research at Emerge Artificial Intelligence Research. You're listening to the AI in Business podcast. This episode is less about near-term business implications and more about the big picture of where AI is taking us and what it means for humanity and human civilization. Our guest this week is Dr. Robin Hansen, Associate Professor of Economics at George Mason University and one of the longstanding thinkers in the sort of singularity space with disagreements with Yadavsky and other major players in that space going back a very, very long time. We've had past series on the AI in Business show called the AI Futures series. We've had famed researchers like Stuart Russell and Stephen Wolfram. We've had leaders from the United Nations and OECD speaking about the policy implications of AI as it becomes strong AI or artificial general intelligence. In this episode with Robin, we talk about when AI becomes strong, in other words, when we have artificial general intelligence, what are likely to be its implications on the international order, particularly that of the United States and China as well as on the economy. Robin and I started this conversation based on sort of staying in touch over the years. My first interview with Robin was 10 long years ago. We follow each other on various social media platforms. I have an article called The Last Words of Alexander which is about artificial general intelligence and sort of its implications in the longer term. Robin had some things he thought he disagreed with about that article and we hash out some of the points where indeed our ideas differ quite substantially. But by the end of this interview, we get down to something that we both agree on which is that even if artificial general intelligence has to be warm, fuzzy and friendly here on earth to keep everybody safe and happy, it damn well better be as strong as possible when it populates the galaxy and has to put up with foes off the planet earth which is very much in line with that essay, The Last Words of Alexander. You can Google my name and then just The Last Words of Alexander. You will find that essay to see what sort of spurned this conversation. I appreciate Robin reaching out to me when he read it and hashing out some interesting topics on the show. It's been a really long time following his thinking and even for some of you who have heard Robin's ideas around artificial general intelligence, I doubt you've seen this much of it unraveled in terms of exactly why he thinks that AGI is not quite as dangerous as some other thinkers think it is and why he thinks it may be an extension of our great kind of technological and economic machine that we're already running here on earth. He makes some analogies that I think are relatively compelling even if you don't agree with all of them. A final point I will mention as we head into this episode is that I have a new podcast coming out called The Trajectory. For those of you who have been here over the years for many, many years, I mean six, seven years running, I've occasionally had these series where we talk about the big picture of artificial general intelligence. When AI goes beyond just making a dent to the bottom line and it starts making a dent to the human condition and maybe to the universe. In this new show called The Trajectory, we're basically going to be hashing out the real politic of artificial general intelligence. What are the power dynamics between the major players, the United States, China, Open AI, DeepMind, et cetera? What are the political moves that they're likely to make? What are the political moves they're making now? What does that mean for sort of who wields power into the future? This is the sole focus of this new show called The Trajectory. This podcast is not yet launched but it will be launched to my new newsletter for The Trajectory which you can learn more about at emerj.com slash TJ1. That's TJ like trajectory, emerj.com slash TJ1 and you can sign up for that newsletter. When that new podcast goes live, you will learn about it. One of our first guests in this Trajectory podcast is none other than Joshua Bengio. Joshua Bengio being the great and famed AI researcher alive today who is now very much an active voice in AI risk. He is one of many amazingly big names that we're going to be bringing on to The Trajectory as soon as that show launches. Stay tuned there and stay tuned for a fun conversation with Robin. Without further ado, let's fly right in. This is Robin Hansen on the AI in Business podcast. Robin Hansen, welcome back to the show. Hello Dan, nice to see you again. Yes, it's great to be able to connect and unpack a topic that I've certainly seen a lot of your tweets about and read your ideas about for many years. In this case, artificial general intelligence and power. I want to start our conversation with some of the things that you really believe people are getting wrong about AGI and power, including kind of controllability and this idea of there being one controlling AI. Can you talk a little bit about what you see as those core mistakes and then also why they're so darn wrong for you? Well, eventually there'll be AGI. I mean, they may not be a single one, there may be thousands of variations. They will of course be more advanced than what we have now, therefore compared to stuff we have now, they will be more powerful, more capable, just like all technology has been getting more capable over time. Just like any other technology, you will want to worry about controlling it or worry about weird things it could do in weird states and watching out for those and preventing them. But that's been true of all technology we've ever had. We have to track it and see whether it's broken or whether it's deviating out of the way we want it. And that will also be true for AGI eventually. There'll be individual AGI systems and people who create them or monitor them or oversee them or regulate them will be interested in watching out for there is dangers they could cause to their owners or their neighbors, things like that. And so it's almost surely that will happen eventually and almost surely there will be people who think about that, who are in the job, task of doing that. But there is this widespread belief that those sort of things would be catastrophic. That is, not only will they deviate from desired parameters, but they might do so in a big sudden way that then they deviate like extremely and then cause enormous damage. And even so far that like one of them will then take over the universe in a sudden moment of deviation from its usual operating parameters. And that's so important that we should be preparing for it and that we can do something now about that. It would be like working out the design of these future systems and their control systems in order to prevent these extreme dysfunctional breaking in this distant future environments. Now, that's the thing I'm more skeptical about that. Sure, there will be future technology and sure it will go wrong sometimes and sure people will be needing to pay some attention to controlling and monitoring it. But in the past, we've always waited until we had concrete examples of systems in front of us behaving in actual environments and we looked for how they actually went wrong. And then we designed control and monitoring systems in order to deal with the actual problem we see in front of us. And that's the reason that makes sense is it's hard to actually anticipate the problems and how to deal with them until you actually see concrete examples. But it's usually worked just fine. That is, systems don't like destroy the world in a moment suddenly after they look to be completely fine. If you're going to destroy the whole world, we usually like have increasingly large damages of whatever you do wrong. And so actual systems, you should be able to see the range of deviations and their causes and guess roughly what the chance of a much larger breakage is and appropriately allocate resources for that. I can certainly see where you're headed here. Why should this be treated as some far off philosophical slash future technical engineering issue of ethics plus the technical parameters plus whatever else when that's really not how we've managed anything else in the real world ever. I can also see that there are certainly folks that are of the belief that a quote unquote fast takeoff may happen after some threshold by which it would be very hard to understand what's going on. And Bostrom has the idea of sort of the black ball being drawn from the urn, right? That at some point there will be one development in AI. Of course it hasn't happened, Robin. You and I are still talking right now. So the ball has not been drawn from the urn. But at some point the ball would be drawn. And that just by happenstance we haven't drawn it yet. But when it is drawn the consequences would be grand. I suspect to the latter circumstance you might say, well, what can you do to predict that anyway? But the former I think has to do with the fact that you are more on the side of the slower takeoff inherently. Is that a misappropriate way of categorizing your thought here? So I definitely think the growth rates of the world economy could be much faster. But if the world economy grew much faster that would be because lots of things could grow fast, not just one part of it. One part of the world growing fast is just not enough to make the whole economy grow fast. So if a fast growing economy counts as a fast takeoff, then I guess I'm on the fast takeoff side. But the scenario is that there's one small part of the world initially of insignificance, tiny power or capability that in a suddenly short time becomes much more powerful than everything else in the universe and takes over everything. That's the scenario I find much less plausible, especially to be working about this far in advance. So you would suspect that we would need all different industries to develop different technologies to develop, AI would be put into good use in all kinds of disparate applications, and there would be that slow bubbling up of everything, including technology, as it has been in the past and so in the future. This is more of your ideas that things will grow in so much as our economy and our needs grow. So certainly the highest meta level thing is let's look at the trajectory of how changes happen in the past, roughly guess change will be roughly like that in the future and prepare accordingly. And now you could entertain arguments that things will be very different in the future than the past, but now you need the burden of proof as you want more to explain what you think different will happen and why. And if you can't make a very good case, I think the rest of us are justified in shrugging our shoulders and saying, yeah, maybe not. And so some people will be familiar with your AI FOOM debates. For those of you who are not, that word is going to be a pretty tough one for you to Google. You can Google AI space FOOM to hear some of Robin's well-known thoughts in this sort of fast versus slow takeoff side of things. I think this ties in some way, shape or form to another misconception that you push back against, which is this idea of, you know, as you framed it, the one AI that takes over the universe, right? The thing that is born at some point, it runs away. It builds rocket ships and conquers the galaxy all by itself as a singular AI. The idea of singleton is brought about. Maybe there's a distinction there that you want to unpack in terms of how you disagree with that idea versus some other idea of super intelligence. Talk to us a little bit about why you really push back against that notion. So let's make sure we understand why it's important. So I think of the example of a coup. So most nations have military systems. And one of the dramatic way in which a military system can fail is through a coup. A nation can literally lose control of its military. And that can be pretty problematic for the nation. It's not necessarily a big problem for the world. So we might think that it's okay to let each nation be in charge of dealing with its coup risk because it faces most of the costs and benefits of that. And a nation that has a coup isn't really going to be that much of a threat to the rest of the world. So in general, if there's a world full of future AIs and whoever is controlling an AI loses control of it, that's going to be a problem for them. And they should anticipate it. But the rest of us don't necessarily have to worry that much about it. The AI they lose control of is a small fraction of the world. And the world will still go on. And they will just have lost a bit. So the reason why you might be more worried about AI risk than that is the scenario that one person losing control of one AI destroys the whole world. And so it has to be more than just a powerful thing you lose control of. It has to be a thing that is incredibly powerful. It's so powerful, it can take over the world. Now the question is that there's thousands of these things. How is it that all of them could take over the world? And now you have to say, well, there's going to be one that's way ahead of the others and then it could take over the world first. Now you have to postulate a scenario not only where there's a thing that could take over the world, but there's one of the proto systems that's way ahead of all the rest. And so you're piling on all these assumptions about the scenario you're worried about. There's this thing in the future that if you lose control, it will take over the entire world, the entire universe, the known universe. And it would do so in a sudden moment that you couldn't see it happening and do something about it. It would be unpredictable and unexpected. And the only way to prevent this thing is to set up some initial control structure for the system. You couldn't really watching it adapt and adapting to how it behaved. You'd have to have some principle design that guaranteed that it couldn't blow up and destroy the world. And you have to think that you could somehow do that for all the systems that ever might be created of that sort. This is a lot of assumptions to have to accept compared to the status quo of how the world's been up until this point. I'm certainly, I don't have a dog in this fight in so much as you do. I think I'm a little bit more middle of the road. Maybe I haven't thought about it as much. I know you've been on this side of the fence for quite some time. I can certainly see the argument that, hey, why wouldn't we expect so many different companies for navigation and logistics and finance and ever to develop extremely intelligent systems to do all sorts of different things? One country invents one to help them set policy. Another country invents one to help them set policy. One invents one to help with autonomous vehicles. Another one does the same thing. And eventually these all bubble up to be really, really smart. Why wouldn't we expect those to all just kind of jockey around? And maybe, Robin, would you expect that those superintelligence systems, maybe if one of them got rowdy or aggressive, the others would kind of push back just kind of as happens in the global system when it comes to war or economics or other things. Is that a safe analogy to make as well in terms of how you would see things? I think so. That is, we have a world of many different power centers, many different nations and firms and professions, et cetera. And no one of them is at much risk of taking over the world. And so if that sort of situation continues, then you should be worried about other risks. I'm with you there. And again, I think that that is pretty plausible. And I can definitely see a future where there's a lot of superintelligence is kind of jockeying in different ways. I think it does also seem somewhat self-evident. And so I can't be black and white here. But there was a time where there was maybe no single cellular life on Earth and then there was single cellular life, so sort of unprecedented thing. And then there were things that wiggled around. And then there were things that were conscious, unprecedented thing, consciousness just emerging from matter. We might imagine that there are some more unprecedented things to come and that we have yet to think of them. You're probably congenial with that idea. Maybe you do think you see it. I'm happy to spread a probability distribution over a large space. There we go. Yeah, yeah. But still, if you said, I have a weird theory and they said Galileo was wrong and therefore I'm right. Well, just because they said Galileo was wrong doesn't mean you're right, right? Sure, sure. But you've got to make the distinction here. I'm not saying I'm right. I'm essentially saying that I'm not going to handcuff myself to either fencepost. Have a vast space of possibilities and distribute a probability distribution over it. But you should pay attention to where in the past what we've seen and maybe it's such a big space you can't really put very large weight on any small part of a big space. So you'll just have to think about a lot of different strange things that could happen. We've done surveys, whether it's AI sentience or artificial general intelligence, it's 30 to 40 PhDs getting ideas. And even then, Robin, I don't know if my ideas are any more confident than they were before. We are talking about the future here and it's kind of tough to juggle that out. If we do have these multiple super intelligences, what you're framing seems to be sort of this future where, which by the way, I think is completely plausible, this future where we have extremely intelligent systems making policy, managing resources, maybe even negotiating with each other in fancy, dancey ways, traveling and gathering resources from far off meteors, whatever the case may be. That sounds like they are serving humanity for the most part, based on at least the way we were kind of articulating it. Would you foresee that, you know, the first wave or maybe even into the foreseeable future, super intelligences would do exactly that? Or would you have a different take? Let's make the analogy with corporations today. We are in a world of super intelligences and that each, most corporations, most government agencies are more capable than anyone human and they exist and they pursue their ends. And we are part of that world, but we are shaped by them and they are shaped by us. Are they at the command of humanity? It's not obvious. It depends on how you want to say it really. But we are thriving and being okay in this new world. I'm not sure we are controlling it. There's a sense in which the world is just not being controlled by any way. The world is on a trajectory. If there's a train going down a track and none of us are really choosing it or controlling it, new technologies appear and that changes the world. We don't vote on that or decide which ones should appear. We are mostly moving into an unexplored space."
  },
  "podcast_summary": "In this podcast episode, Daniel Fijell interviews Dr. Robin Hanson, an Associate Professor of Economics at George Mason University and a longstanding thinker",
  "podcast_guest": "",
  "podcast_highlights": ""
}
